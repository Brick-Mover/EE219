{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categories = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "\n",
    "\n",
    "class mytokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = re.sub(r'[^A-Za-z]', \" \", text)\n",
    "        tokens = re.sub(\"[,.-:/()?{}*$#&]\",\" \",tokens)\n",
    "        tokens =[word for tk in nltk.sent_tokenize(tokens) for word in nltk.word_tokenize(tk)]\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]{2,}', token):\n",
    "                new_tokens.append(token)     \n",
    "        stems = [self.stemmer.stem(t) for t in new_tokens]\n",
    "        return stems\n",
    "\n",
    "\n",
    "class Project1(object):\n",
    "\n",
    "    def __init__(self, minDf):\n",
    "        self.eightTrainingData = None\n",
    "        self.eightTestingData = None\n",
    "        self.minDf = minDf\n",
    "        self.XTrainingCount = None\n",
    "        self.XTrainTfidf = None\n",
    "        self.XTestingCount = None\n",
    "        self.XTestTfidf = None\n",
    "        self.countVec = None\n",
    "        self.XLSITraining = None\n",
    "        self.yLSITraining = None\n",
    "        self.XNMFTraining = None\n",
    "        self.yNMFTraining = None\n",
    "        self.XLSITesting = None\n",
    "        self.yLSITesting = None\n",
    "        self.XNMFTesting = None\n",
    "        self.yNMFTesting = None\n",
    "\n",
    "    def load8TestingData(self):\n",
    "        if self.eightTestingData is None:\n",
    "            self.eightTestingData = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                                                    remove=('headers','footers','quotes'))\n",
    "\n",
    "    def load8TrainingData(self):\n",
    "        if self.eightTrainingData is None:\n",
    "            self.eightTrainingData = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                                    remove=('headers','footers','quotes'))\n",
    "    def createXTrainingCounts(self):\n",
    "        if self.countVec is None:\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('averaged_perceptron_tagger')\n",
    "            self.countVec = CountVectorizer(min_df=self.minDf, analyzer='word',\n",
    "                                            stop_words=text.ENGLISH_STOP_WORDS, tokenizer=mytokenizer())\n",
    "        self.load8TrainingData()\n",
    "        if self.XTrainingCount is None:\n",
    "            self.XTrainingCount = self.countVec.fit_transform(self.eightTrainingData.data)\n",
    "\n",
    "    def createXTestingCounts(self):\n",
    "        if self.XTrainingCount is None:\n",
    "            self.createXTrainingCounts()\n",
    "\n",
    "        print('XTrainingCount size is %s' % (self.XTrainingCount.shape, ))\n",
    "\n",
    "        self.XTestingCount = self.countVec.transform(self.eightTestingData.data)\n",
    "        print('XTestingCount size is %s' % (self.XTestingCount.shape, ))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    (a) Plot a histogram of the number of training documents per class to check if they are evenly distributed.\n",
    "    \"\"\"\n",
    "    def problemA(self):\n",
    "        self.plot_size()\n",
    "\n",
    "    def plot_size(self):\n",
    "        for category in categories:\n",
    "            trainingData = fetch_20newsgroups(subset='train', categories=[category], remove=('headers','footers','quotes'))\n",
    "            print(category, len(trainingData.filenames))\n",
    "\n",
    "    \"\"\"\n",
    "    (b) Modeling Text Data: tokenize each document into words. Then, excluding the stop words,\n",
    "    punctuations, and using stemmed version of words, create a TFxIDF vector representations.\n",
    "    min_df = 2 or 5\n",
    "    \"\"\"\n",
    "\n",
    "    def problemB(self):\n",
    "        self.model_text_data()\n",
    "\n",
    "    def model_text_data(self):\n",
    "        # load data\n",
    "        self.load8TrainingData()\n",
    "        # tokenization\n",
    "        self.createXTrainingCounts()\n",
    "\n",
    "        print('Size of feature vectors when minDf is %s: %s' % (self.minDf, self.XTrainingCount.shape))\n",
    "\n",
    "        # compute tf-idf\n",
    "        tfidfTransformer = TfidfTransformer()\n",
    "\n",
    "        if self.XTrainTfidf is None:\n",
    "            self.XTrainTfidf = tfidfTransformer.fit_transform(self.XTrainingCount)\n",
    "        print('Size of tf-idf when minDf is %s: %s' % (self.minDf, self.XTrainTfidf.shape))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    (c) Report 10 most significant terms base on tf-icf\n",
    "    \"\"\"\n",
    "    def problemC(self):\n",
    "        classes = ['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "                   'misc.forsale', 'soc.religion.christian']\n",
    "        tfIcf, features = self.calc_tf_icf(classes)\n",
    "        for i in range(0, 4):\n",
    "            result = tfIcf[i, :]\n",
    "            idx = np.argpartition(result, -10)[-10:]\n",
    "            idx = idx[np.argsort(result[idx])]\n",
    "            print(\"top 10 feature of %s:\" % (classes[i])),\n",
    "            print([features[i] for i in idx])\n",
    "\n",
    "\n",
    "    def calc_tf_icf(self, classes):\n",
    "        if self.countVec is None:\n",
    "            self.countVec = CountVectorizer(min_df=self.minDf, analyzer='word',\n",
    "                                            stop_words=text.ENGLISH_STOP_WORDS, tokenizer=mytokenizer())\n",
    "\n",
    "        features = set()\n",
    "        for c in classes:\n",
    "            CTrainingData = fetch_20newsgroups(subset='train', categories=[c])\n",
    "            self.countVec.fit_transform(CTrainingData.data)\n",
    "            Cword = set(self.countVec.vocabulary_.keys())\n",
    "            features |= Cword\n",
    "\n",
    "        features = list(features)\n",
    "        word2Idx = {}\n",
    "        for idx, word in enumerate(features):\n",
    "            word2Idx[word] = idx\n",
    "\n",
    "        print(\"number of words we care: \", len(features))\n",
    "\n",
    "        # tf_icf = Matrix(#class, #words)\n",
    "        tf = np.zeros(shape=(len(classes), len(features)))\n",
    "        cf = np.zeros(shape=(1, len(features)))\n",
    "\n",
    "        # iterate through the four classes to get term frequency\n",
    "        for cIdx, c in enumerate(classes):\n",
    "            CData = fetch_20newsgroups(subset='train', categories=[c])\n",
    "            CwordCountSum = self.countVec.fit_transform(CData.data).sum(axis=0)\n",
    "            Cword = self.countVec.get_feature_names()\n",
    "            for idx, word in enumerate(Cword):\n",
    "                tf[cIdx, word2Idx[word]] += CwordCountSum[0, idx]\n",
    "\n",
    "        for c in list(fetch_20newsgroups(subset='train').target_names):\n",
    "            CData = fetch_20newsgroups(subset='train', categories=[c])\n",
    "            CwordCountSum = self.countVec.fit_transform(CData.data).sum(axis=0)\n",
    "            Cword = self.countVec.get_feature_names()\n",
    "            for idx, word in enumerate(Cword):\n",
    "                if word in word2Idx and CwordCountSum[0, idx] > 0:\n",
    "                    cf[0, word2Idx[word]] += 1\n",
    "\n",
    "        cf[cf == 0] = 1\n",
    "        icf = np.log2(20 / cf) + 1\n",
    "\n",
    "        return tf * icf, features\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    (d) Apply LSI and NMF to TF*IDF\n",
    "    \"\"\"\n",
    "    def problemD(self):\n",
    "        # load data\n",
    "        self.load8TrainingData()\n",
    "        # tokenization\n",
    "        self.createXTrainingCounts()\n",
    "\n",
    "        print('Size of feature vectors when minDf is %s: %s' % (self.minDf, self.XTrainingCount.shape))\n",
    "\n",
    "        # compute tf-idf\n",
    "        tfidfTransformer = TfidfTransformer()\n",
    "\n",
    "        if self.XTrainTfidf is None:\n",
    "            self.XTrainTfidf = tfidfTransformer.fit_transform(self.XTrainingCount)\n",
    "        print('Size of train tf-idf when minDf is %s: %s' % (self.minDf, self.XTrainTfidf.shape))\n",
    "\n",
    "        svd = TruncatedSVD(n_components=50)\n",
    "        self.XLSITraining = svd.fit_transform(self.XTrainTfidf)\n",
    "        self.yLSITraining = [ int(x / 4) for x in self.eightTrainingData.target ]\n",
    "\n",
    "        nmf = NMF(n_components=50)\n",
    "        self.XNMFTraining = nmf.fit_transform(self.XTrainTfidf)\n",
    "        self.yNMFTraining = self.yLSITraining\n",
    "\n",
    "        #\n",
    "        # apply LSI and NMF to testing data\n",
    "        #\n",
    "\n",
    "        # load test data\n",
    "        self.load8TestingData()\n",
    "        # tokenization\n",
    "        self.createXTestingCounts()\n",
    "\n",
    "        if self.XTestTfidf is None:\n",
    "            self.XTestTfidf = tfidfTransformer.transform(self.XTestingCount)\n",
    "        print('Size of test tf-idf when minDf is %s: %s' % (self.minDf, self.XTestTfidf.shape))\n",
    "\n",
    "        self.XLSITesting = svd.transform(self.XTestTfidf)\n",
    "        self.yLSITesting = [ int(x / 4) for x in self.eightTestingData.target ]\n",
    "\n",
    "        self.XNMFTesting = nmf.transform(self.XTestTfidf)\n",
    "        self.yNMFTesting = self.yLSITesting\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    (e) Use hard margin classifier to separate the\n",
    "    documents into ‘Computer Technology’ vs ‘Recreational Activity’ groups\n",
    "    \"\"\"\n",
    "    def problemE(self, method, penalty):\n",
    "        if self.XLSITraining is None or self.yLSITraining is None or\\\n",
    "            self.XNMFTraining is None or self.yNMFTraining is None:\n",
    "            self.problemD()     # will have to use everything in part D\n",
    "\n",
    "        self.load8TestingData()\n",
    "\n",
    "        assert penalty == \"hard\" or penalty == \"soft\"\n",
    "        if penalty == \"hard\":\n",
    "            lSVC = svm.LinearSVC(C=1000)\n",
    "        elif penalty == 'soft':\n",
    "            lSVC = svm.LinearSVC(C=0.001)\n",
    "        else:\n",
    "            lSVC = svm.LinearSVC(C=penalty)\n",
    "\n",
    "        assert method == \"LSI\" or method == \"NMF\"\n",
    "        if method == \"LSI\":\n",
    "            XTrain, yTrain, XTest, yTest =\\\n",
    "                self.XLSITraining, self.yLSITraining, self.XLSITesting, self.yLSITesting\n",
    "        else:\n",
    "            XTrain, yTrain, XTest, yTest =\\\n",
    "                self.XNMFTraining, self.yNMFTraining, self.XNMFTesting, self.yNMFTesting\n",
    "\n",
    "        lSVC.fit(XTrain, yTrain)\n",
    "        yScore = lSVC.decision_function(XTest)\n",
    "\n",
    "        #plot ROC\n",
    "        self.plot_ROC(yScore, 'LSI')\n",
    "        plt.savefig('fig/roc_%s_%s_df%d.png' % (method, penalty, self.minDf), bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        #plot confusion matrix\n",
    "        class_names = ['Com Tech', 'Recreation']\n",
    "        svm_pred = lSVC.predict(XTest)\n",
    "        conf_mat = confusion_matrix(yTest, svm_pred)\n",
    "\n",
    "        self.plot_confusion_matrix(conf_mat, classname=class_names, title='Confusion matrix')\n",
    "        plt.savefig('fig/conf_mat_%s_%s_df%d.png' %\n",
    "                    (method, penalty, self.minDf), bbox_inches='tight')\n",
    "\n",
    "        self.plot_confusion_matrix(conf_mat, classname=class_names, normalize=True, \n",
    "                                        title='Normalized confusion matrix')\n",
    "        plt.savefig('fig/conf_mat_norm_%s_%s_df%d.png' %\n",
    "                    (method, penalty, self.minDf), bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # accuracy\n",
    "        svm_accuracy = accuracy_score(yTest, svm_pred)\n",
    "        print('SVM accuracy for %s and %s Margin with df %d is: %s' %\n",
    "              (method, penalty, self.minDf, str(svm_accuracy)))\n",
    "\n",
    "        # recall\n",
    "        svm_recall = recall_score(yTest, svm_pred)\n",
    "        print('SVM recall for %s and %s Margin with df %d is: %s' %\n",
    "              (method, penalty, self.minDf, str(svm_recall)))\n",
    "\n",
    "        # precision\n",
    "        svm_precision = precision_score(yTest, svm_pred)\n",
    "        print('SVM precision for %s and %s Margin with df %s is: %s' %\n",
    "              (method, penalty, str(self.minDf), str(svm_precision)))\n",
    "\n",
    "\n",
    "    def plot_ROC(self, yScore, method):\n",
    "        assert method == \"LSI\" or method == \"NMF\"\n",
    "        if method == \"LSI\":\n",
    "            fpr, tpr, thresholds = roc_curve(self.yLSITesting, yScore)\n",
    "        elif method == \"NMF\":\n",
    "            fpr, tpr, thresholds = roc_curve(self.yNMFTesting, yScore)\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        print(len(fpr))\n",
    "        print(len(tpr))\n",
    "        print(len(thresholds))\n",
    "\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # make confusion matrix plot\n",
    "    def plot_confusion_matrix(self, cmat, classname, normalize=False, title='Confusion matrix'):\n",
    "        plt.figure()\n",
    "        cmap = plt.cm.Blues\n",
    "        plt.imshow(cmat, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "\n",
    "        tick_marks = np.arange(len(classname))\n",
    "        plt.xticks(tick_marks, classname, rotation=45)\n",
    "        plt.yticks(tick_marks, classname)\n",
    "\n",
    "        if normalize:\n",
    "            cmat = cmat.astype('float') / cmat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        print(cmat)\n",
    "\n",
    "        thresh = cmat.max() / 2.\n",
    "        for i, j in itertools.product(range(cmat.shape[0]), range(cmat.shape[1])):\n",
    "            plt.text(j, i, \"%.2f\"%cmat[i, j], horizontalalignment=\"center\", color=\"white\" if cmat[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "    \"\"\"\n",
    "    (f) Use a 5-fold cross-validation to find the best value of the parameter\n",
    "    \"\"\"\n",
    "    def problemF(self, method):\n",
    "\n",
    "        if self.XLSITraining is None or self.yLSITraining is None or self.XNMFTraining is None or self.yNMFTraining is None:\n",
    "            self.problemD()     # will have to use everything in part D\n",
    "\n",
    "        if method == \"LSI\":\n",
    "            XTrain, yTrain, XTest, yTest =\\\n",
    "                self.XLSITraining, self.yLSITraining, self.XLSITesting, self.yLSITesting\n",
    "        else:\n",
    "            XTrain, yTrain, XTest, yTest =\\\n",
    "                self.XNMFTraining, self.yNMFTraining, self.XNMFTesting, self.yNMFTesting\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        matrix = [[0]*7 for i in range(5)]\n",
    "\n",
    "        #\n",
    "        # LSI\n",
    "        #\n",
    "        i = 0\n",
    "        for train_index, test_index in kf.split(XTrain):\n",
    "            X_train, X_test = XTrain[train_index], XTrain[test_index]\n",
    "            j = 0\n",
    "            for k in [-3, -2, -1, 0, 1, 2, 3]:\n",
    "                y_train = [ int(x / 4) for x in self.eightTrainingData.target[train_index]]    \n",
    "                y_test = [ int(x / 4) for x in self.eightTrainingData.target[test_index]]    \n",
    "                SVC = svm.LinearSVC(C=10**k)\n",
    "                SVC.fit(X_train, y_train)\n",
    "                score = SVC.score(X_test, y_test)\n",
    "                matrix[i][j]=score\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "\n",
    "        avg_value = np.array(matrix)\n",
    "        print (avg_value.shape)\n",
    "\n",
    "        max = 0\n",
    "        max_index = 0\n",
    "        for i in range (7):\n",
    "            mean = np.mean(avg_value[:,i:i+1])\n",
    "            if max < mean:\n",
    "                max = mean\n",
    "                max_index = i\n",
    "        print (max, max_index)\n",
    "        penalty = [-3, -2, -1, 0, 1, 2, 3]\n",
    "        print ('The best penalty value for '+str(method)+' method is',10**penalty[max_index]) \n",
    "\n",
    "        self.problemE(method, 10**penalty[max_index])\n",
    "\n",
    "\n",
    "    def problemGH(self, classifier, method, penalty=\"l2\", reg=1):\n",
    "        if self.XLSITraining is None or self.yLSITraining is None or\\\n",
    "            self.XNMFTraining is None or self.yNMFTraining is None:\n",
    "            self.problemD()     # will have to use everything in part D\n",
    "\n",
    "        assert classifier == \"MultiNB\" or classifier == \"Logi\"\n",
    "        if classifier == \"MultiNB\":\n",
    "            clf = GaussianNB()\n",
    "        else:\n",
    "            clf = LogisticRegression(penalty=penalty, C=reg)\n",
    "\n",
    "        assert method == \"LSI\" or method == \"NMF\"\n",
    "        if method == \"LSI\":\n",
    "            XTrain, yTrain, XTest, yTest =\\\n",
    "                self.XLSITraining, self.yLSITraining, self.XLSITesting, self.yLSITesting\n",
    "        else:\n",
    "            XTrain, yTrain, XTest, yTest =\\\n",
    "                self.XNMFTraining, self.yNMFTraining, self.XNMFTesting, self.yNMFTesting\n",
    "\n",
    "        clf.fit(XTrain, yTrain)\n",
    "        if classifier == \"MultiNB\":\n",
    "            yScore = clf.predict(XTest)\n",
    "        else:\n",
    "            yScore = clf.decision_function(XTest)\n",
    "        self.plot_ROC(yScore, method)\n",
    "        plt.savefig('fig/roc_%s_%s_penalty_%s_reg_%s_df%d.png' %\n",
    "                    (classifier, method, penalty, str(reg), self.minDf), bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def problemI(self, method):\n",
    "        if self.XLSITraining is None or self.yLSITraining is None or\\\n",
    "                self.XNMFTraining is None or self.yNMFTraining is None:\n",
    "            self.problemD()     # will have to use everything in part D\n",
    "\n",
    "        assert method == \"LSI\" or method == \"NMF\"\n",
    "        if method == \"LSI\":\n",
    "            XTrain, yTrain, XTest, yTest =\\\n",
    "                self.XLSITraining, self.yLSITraining, self.XLSITesting, self.yLSITesting\n",
    "        else:\n",
    "            XTrain, yTrain, XTest, yTest =\\\n",
    "                self.XNMFTraining, self.yNMFTraining, self.XNMFTesting, self.yNMFTesting\n",
    "\n",
    "        for penalty in [\"l1\", \"l2\"]:\n",
    "            for reg in [0.01, 0.1, 1, 10, 100, 1000]:\n",
    "                self.problemGH(\"Logi\", \"LSI\", penalty, reg)\n",
    "\n",
    "\n",
    "    def fetch_data(self, subset, cate):\n",
    "        data = fetch_20newsgroups(subset=subset, categories=cate, shuffle=True,\n",
    "                                    random_state=42, remove=('headers','footers','quotes'))\n",
    "        return data\n",
    "\n",
    "    def dim_red(self, method, data):\n",
    "        if self.countVec is None:\n",
    "            self.countVec = CountVectorizer(min_df=self.minDf, analyzer='word',\n",
    "                                            stop_words=text.ENGLISH_STOP_WORDS, tokenizer=mytokenizer())\n",
    "        count_vec = self.countVec.fit_transform(data.data)\n",
    "        tfidf = TfidfTransformer().fit_transform(count_vec)\n",
    "\n",
    "        assert method == \"LSI\" or method == \"NMF\"\n",
    "        if method == 'LSI':\n",
    "            svd = TruncatedSVD(n_components=50)\n",
    "            red_mat = svd.fit_transform(tfidf)\n",
    "        else:\n",
    "            nmf = NMF(n_components=50)\n",
    "            red_mat = nmf.fit_transform(tfidf)\n",
    "\n",
    "        return red_mat\n",
    "\n",
    "    def problemJ(self, method, classifier, class_method='OneOne'):\n",
    "\n",
    "        categories_j = ['comp.sys.ibm.pc.hardware','comp.sys.mac.hardware',\n",
    "                        'misc.forsale','soc.religion.christian']\n",
    "        XTrainData = self.fetch_data('train', categories_j)\n",
    "        XTestData = self.fetch_data('test', categories_j)\n",
    "        XTrain = self.dim_red(method, XTrainData)\n",
    "        XTest = self.dim_red(method, XTestData)\n",
    "\n",
    "        assert classifier == 'NB' or classifier == 'SVM'\n",
    "        if classifier == 'NB':\n",
    "            clf = GaussianNB().fit(XTrain, XTrainData.target)\n",
    "            name_insert = str(method)+'_'+str(classifier)+'_df'+str(self.minDf)\n",
    "        else:\n",
    "            assert class_method == 'OneOne' or class_method == 'OneRest'\n",
    "            if class_method == 'OneOne':\n",
    "                clf = OneVsOneClassifier(svm.LinearSVC(C=10, random_state=42)).fit(XTrain, XTrainData.target)\n",
    "                name_insert = str(method)+'_'+str(classifier)+'_'+str(class_method)+'_df'+str(self.minDf)\n",
    "            else:\n",
    "                clf = OneVsRestClassifier(svm.LinearSVC(C=10, random_state=42)).fit(XTrain, XTrainData.target)\n",
    "                name_insert = str(method)+'_'+str(classifier)+'_'+str(class_method)+'_df'+str(self.minDf)\n",
    "\n",
    "        pred = clf.predict(XTest)\n",
    "\n",
    "        class_names = ['pc_hardware', 'mac_hardware', 'misc_forsale', 'religion_christian']\n",
    "        conf_mat = confusion_matrix(XTestData.target, pred)\n",
    "\n",
    "        self.plot_confusion_matrix(conf_mat, classname=class_names,\n",
    "                              title='Confusion matrix')\n",
    "\n",
    "        plt.savefig('fig/conf_mat_%s.png' % name_insert, bbox_inches='tight')       \n",
    "        self.plot_confusion_matrix(conf_mat, classname=class_names, normalize=True,\n",
    "                              title='Normalized confusion matrix')\n",
    "        plt.savefig('fig/conf_mat_norm_%s.png' % name_insert, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # accuracy\n",
    "        accuracy = accuracy_score(XTestData.target, pred)\n",
    "        print('Accuracy for %s is: %s' % (name_insert, str(accuracy)))\n",
    "\n",
    "        # recall\n",
    "        recall = recall_score(XTestData.target, pred, average='weighted')\n",
    "        print('Recall for %s is: %s' % (name_insert, str(recall)))\n",
    "\n",
    "        # precision\n",
    "        precision = precision_score(XTestData.target, pred, average='weighted')\n",
    "        print('Precision for %s is: %s' % (name_insert, str(precision)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # debug mytokenizer (spam)\n",
    "    # corpus = ['stem stems stemming, go stemmed']\n",
    "    # # a = Project1(minDf=2)\n",
    "    # # countVec = CountVectorizer(stop_words=text.ENGLISH_STOP_WORDS, tokenizer=a.mytokenizer)\n",
    "    # countVec = CountVectorizer(stop_words=text.ENGLISH_STOP_WORDS, tokenizer=mytokenizer())\n",
    "    # out = countVec.fit_transform(corpus)\n",
    "    # name = countVec.get_feature_names()\n",
    "    # print(name)\n",
    "\n",
    "    #\n",
    "    p = Project1(minDf=2)\n",
    "    # p.problemA()\n",
    "    # p.problemB()\n",
    "    # p.problemC()\n",
    "    # p.problemD()\n",
    "    # p.problemE(\"LSI\", \"hard\")\n",
    "    # p.problemE(\"LSI\", \"soft\")\n",
    "    # p.problemE(\"NMF\", \"hard\")\n",
    "    # p.problemE(\"NMF\", \"soft\")\n",
    "    # p.problemF()\n",
    "    # p.problemGH(\"MultiNB\", \"LSI\")\n",
    "    p.problemI(\"LSI\")\n",
    "\n",
    "    # p.problemF('LSI')\n",
    "    # p.problemF('NMF')\n",
    "    # p.problemGH()\n",
    "    # p.problemGH(\"MultiNB\", \"LSI\")\n",
    "    p.problemJ('LSI', 'NB')\n",
    "    p.problemJ('LSI', 'SVM', 'OneOne')\n",
    "    p.problemJ('LSI', 'SVM', 'OneRest')\n",
    "    p.problemJ('NMF', 'NB')\n",
    "    p.problemJ('NMF', 'SVM', 'OneOne')\n",
    "    p.problemJ('NMF', 'SVM', 'OneRest')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
